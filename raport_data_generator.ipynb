{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praca domowa nr 4 - generator danych do raportu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wojciech Celej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Załadowanie zbiorów testowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generacja słownika `benchmark_set` przechowującego tabele opisujące dany zbiór benchmarkowy. Klucz dla danego zbioru jest tworzony według schematu: `<nazwa_folderu>_<nazwa_zbioru>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"pd4-zbiory-benchmarkowe\"\n",
    "data_suffix = \".data.gz\"\n",
    "label_suffix = \".labels0.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_list = [f for f in os.listdir(dataset_directory) if os.path.isdir(os.path.join(dataset_directory, f)) and not f.startswith(\".\")]\n",
    "benchmark_set = {}\n",
    "for directory in directories_list:\n",
    "    path_to_dataset = os.path.join(dataset_directory, directory)\n",
    "    for file in os.listdir(path_to_dataset):\n",
    "        if file.endswith(data_suffix):\n",
    "            data = np.loadtxt(os.path.join(path_to_dataset, file), ndmin=2)\n",
    "            data_set_name = file.split(\".\")[0]\n",
    "            label_file_name = data_set_name + label_suffix\n",
    "            label = np.loadtxt(os.path.join(path_to_dataset, label_file_name), dtype=np.int)\n",
    "            d = {\"x\": data[:, 0], \"y\": data[:, 1], \"label\": label}\n",
    "            df = pd.DataFrame(data=d)            \n",
    "            benchmark_set[directory+\"_\"+data_set_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak utworzone zbiory można narysować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in benchmark_set.items():\n",
    "    d = value\n",
    "    ax = sns.scatterplot(data=d, x=d.columns[0], y=d.columns[1], hue=d.columns[2], legend=\"full\", palette=\"tab20\")\n",
    "    ax.set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spectral\n",
    "import scipy as sp\n",
    "import genieclust\n",
    "import sklearn.cluster\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie algorytmów klasteryzacji na poszczególnych zbiorach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytmy, które zostaną zbadane:\n",
    "* algorytm spektralny (włsana implementacja)\n",
    "* algorytmy hierarchiczne z pakietu `scipy.cluster.hierarchy.linkage`\n",
    "* algorytm *Genie* z pakietu `genieclust`\n",
    "* 3 algorytmy pochodzące z `sklearn.cluster`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzona zostanie tabela `DataFrame`, gdzie każdy jej wiersz będzie zawierał:\n",
    "* nazwę zbioru\n",
    "* nazwę użytego algorytmu  \n",
    "oraz wartości:\n",
    "* indeksu Fowlkesa-Mallowsa (FM): `sklearn.metrics.fowlkes_mallows_score()`\n",
    "* indeksu AM: `sklearn.metrics.adjusted_mutual_info_score()`\n",
    "* skorygowanego indeksu Randa (AR): `sklearn.metrics.adjusted_rand_score()`  \n",
    "* analogiczne 3 wartości indeksów dla zmiennych ustandaryzowanych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = (\"benchmark_set\", \"algorithm\", \"FM\", \"AM\", \"AR\", \"FM_std\", \"AM_std\", \"AR_std\")\n",
    "results = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_row(df: pd.DataFrame, row):\n",
    "    \"\"\"\n",
    "    df - DataFrame to which append\n",
    "    row - tuple or list containing row values - must be in proper order\n",
    "    return: new DataFrame object\n",
    "    \"\"\"\n",
    "    if len(df.columns) != len(row):\n",
    "        raise ValueError\n",
    "    a = dict()\n",
    "    for i in range(len(row)):\n",
    "        a[df.columns[i]] = row[i]\n",
    "    new_df = df.append(a, ignore_index=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_point_df(df):\n",
    "    X = df.loc[:, [\"x\", \"y\"]].values\n",
    "    X_std = (X-np.mean(X, axis=0))/np.std(X, ddof=1, axis=0)\n",
    "    labels_true = df.loc[:, \"label\"].values\n",
    "    numOfClusters = df.loc[:, \"label\"].unique().shape[0]\n",
    "    return X, X_std, labels_true, numOfClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_perf_evaluation(labels_true, labels_pred):\n",
    "    fm = fowlkes_mallows_score(labels_true, labels_pred)\n",
    "    am = adjusted_mutual_info_score(labels_true, labels_pred, average_method=\"arithmetic\")\n",
    "    ar = adjusted_rand_score(labels_true, labels_pred)\n",
    "    assert np.all(labels_pred>=0)\n",
    "    return fm, am, ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Własna implementacja algorytmu spektralnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neigh in n_neighbors:\n",
    "    for set_name, set_data in benchmark_set.items():\n",
    "        if not set_name.startswith(\"sipu_unbal\"):\n",
    "            continue\n",
    "        X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "        labels_pred = spectral.spectral_clustering(X, numOfClusters, n_neigh)\n",
    "        labels_pred2 = spectral.spectral_clustering(X_std, numOfClusters, n_neigh)\n",
    "        fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "        fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "        row_values = (set_name, \"spectral_n_neigh_\"+str(n_neigh), fm, am, ar, fm2, am2, ar2)\n",
    "        results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorytmy hierarchiczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_methods = [\"single\", \"complete\", \"average\", \"weighted\", \"centroid\", \"median\", \"ward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in hierarchical_methods:\n",
    "    for set_name, set_data in benchmark_set.items():\n",
    "        X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "        z = sp.cluster.hierarchy.linkage(X, method=method)\n",
    "        labels_pred = sp.cluster.hierarchy.cut_tree(z, n_clusters=numOfClusters).reshape(-1)\n",
    "        z = sp.cluster.hierarchy.linkage(X_std, method=method)\n",
    "        labels_pred2 = sp.cluster.hierarchy.cut_tree(z, n_clusters=numOfClusters).reshape(-1)\n",
    "        fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "        fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "        row_values = (set_name, \"hierarchy_\"+method, fm, am, ar, fm2, am2, ar2)\n",
    "        results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorytm *Genie*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_tresholds = [0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treshold in gini_tresholds:\n",
    "    for set_name, set_data in benchmark_set.items():\n",
    "        X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "        g = genieclust.genie.Genie(n_clusters=numOfClusters, gini_threshold=treshold)\n",
    "        labels_pred = g.fit_predict(X)\n",
    "        labels_pred2 = g.fit_predict(X_std)\n",
    "        fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "        fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "        row_values = (set_name, \"genieclust_tresh_\"+str(treshold), fm, am, ar, fm2, am2, ar2)\n",
    "        results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wybrane 3 Algorytmy z pakietu `sklearn.cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_name, set_data in benchmark_set.items():\n",
    "    X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "    g = sklearn.cluster.KMeans(n_clusters=numOfClusters)\n",
    "    labels_pred = g.fit_predict(X)\n",
    "    labels_pred2 = g.fit_predict(X_std)\n",
    "    fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "    fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "    row_values = (set_name, \"KMeans\", fm, am, ar, fm2, am2, ar2)\n",
    "    results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damp_values = [0.7, 0.9]\n",
    "for damp in damp_values:\n",
    "    for set_name, set_data in benchmark_set.items():\n",
    "        if not set_name.startswith(\"sipu_\"):\n",
    "            continue\n",
    "        X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "        g = sklearn.cluster.AffinityPropagation(damping=damp)\n",
    "        labels_pred = g.fit_predict(X)\n",
    "        labels_pred2 = g.fit_predict(X_std)\n",
    "        fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "        fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "        row_values = (set_name, \"AffProp_damp_\"+str(damp), fm, am, ar, fm2, am2, ar2)\n",
    "        results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = [0.4, 0.3, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in threshold_values:\n",
    "    for set_name, set_data in benchmark_set.items():\n",
    "        X, X_std, labels_true, numOfClusters = parse_point_df(set_data)\n",
    "        g = sklearn.cluster.Birch(n_clusters=numOfClusters, threshold=threshold)\n",
    "        labels_pred = g.fit_predict(X)\n",
    "        labels_pred2 = g.fit_predict(X_std)\n",
    "        fm, am, ar = clustering_perf_evaluation(labels_true, labels_pred)\n",
    "        fm2, am2, ar2 = clustering_perf_evaluation(labels_true, labels_pred2)\n",
    "        row_values = (set_name, \"Birch_thresh_\"+str(threshold), fm, am, ar, fm2, am2, ar2)\n",
    "        results = append_row(results, row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapisanie wyników do pliku `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
